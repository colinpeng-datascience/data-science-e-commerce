{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import style\n",
    "\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-major",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-martin",
   "metadata": {},
   "source": [
    "This notebook contains a model I made from scratch to evaluate if a keyword(short sentence or a word) and a sentence( a long  sentence or a paragraph) is a match. I used two encoder RNNs, one for the keyword and one for the description, plus a Attention-like mechanism, but not as complicated as Attention.(see the model part for more detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-salvation",
   "metadata": {},
   "source": [
    "# Define classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-alliance",
   "metadata": {},
   "source": [
    "### Tool classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use(\"ggplot\")\n",
    "plt.switch_backend('agg')\n",
    "def showPlot(points):\n",
    "    %matplotlib inline\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-ridge",
   "metadata": {},
   "source": [
    "Geting a list of words from a sentence. This function is typically useful for ecommerce sites that use chinese as their main language. Note that one can modify this part according to her needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(s1):\n",
    "    regEx = re.compile('([\\u4e00-\\u9fa5]|[^a-zA-Z0-9_-]+)') \n",
    "    res = re.compile(r\"([\\u4e00-\\u9fa5])\")\n",
    "    p1 = regEx.split(str(s1).lower())\n",
    "    str1_list = []\n",
    "    for stri in p1:\n",
    "        \n",
    "        if res.split(stri) == None:\n",
    "            str1_list.append(stri)\n",
    "        else:\n",
    "            ret = res.split(stri)\n",
    "            for ch in ret:\n",
    "                str1_list.append(ch)\n",
    "\n",
    "    list_word1 = [w for w in str1_list if len(w) != 0] \n",
    "\n",
    "    return  list_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-miracle",
   "metadata": {},
   "source": [
    "### Dataset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-wheat",
   "metadata": {},
   "source": [
    "I defined my own data class to store data in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_ch(text):\n",
    "        return get_word_list(text)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_ch(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_ch(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, freq_threshold=5,vocab = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "\n",
    "        # Get key, des, score columns\n",
    "        self.key = self.df[\"pre_search_word\"]\n",
    "        self.desc = self.df[\"name\"]\n",
    "        self.score = self.df[\"y\"]\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "        if vocab == None:\n",
    "            self.vocab = Vocabulary(freq_threshold)\n",
    "            self.vocab.build_vocabulary(self.desc.tolist()+self.key.tolist())\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = self.key[index]\n",
    "        desc = self.desc[index]\n",
    "        score = self.score[index]\n",
    "        \n",
    "        numericalized_key = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_key += self.vocab.numericalize(key)\n",
    "        numericalized_key.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        numericalized_desc = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_desc += self.vocab.numericalize(desc)\n",
    "        numericalized_desc.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        \n",
    "        return torch.tensor(score), torch.tensor(numericalized_key), torch.tensor(numericalized_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        scores = [item[0] for item in batch]\n",
    "        scores = torch.tensor(scores)\n",
    "        \n",
    "        keys = [item[1] for item in batch]\n",
    "        keys = pad_sequence(keys, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        descs = [item[2] for item in batch]\n",
    "        descs = pad_sequence(descs, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        \n",
    "\n",
    "        return scores, keys, descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader( root_folder,\n",
    "                file_name,\n",
    "                batch_size=32,\n",
    "                num_workers=8,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                dataset = None,\n",
    "                start_from = 0,\n",
    "                vocab = None):\n",
    "    if dataset == None:\n",
    "        dataset = MyDataset(root_folder, file_name,vocab = vocab)\n",
    "        \n",
    "    if start_from != 0:\n",
    "        dataset.df = dataset.df[start_from:].reset_index(drop=True)\n",
    "        dataset.key = dataset.key[start_from:].reset_index(drop=True)\n",
    "        dataset.desc = dataset.desc[start_from:].reset_index(drop=True)\n",
    "        dataset.score = dataset.score[start_from:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_load_data(batch_size = 1,from_csv_file = False,save = False,csv_file = \"train.csv\",start_from = 0, vocab = None):\n",
    "\n",
    "    if from_csv_file == False:\n",
    "        with open('dataloader.pkl', 'rb') as input:\n",
    "            dataset = pickle.load(input)\n",
    "            data_start = pickle.load(input)\n",
    "        train_loader, dataset = get_loader(\"\", csv_file ,batch_size = batch_size,dataset = dataset,start_from = start_from-data_start)\n",
    "        \n",
    "    else:\n",
    "        train_loader, dataset = get_loader(\"\", csv_file ,batch_size = batch_size,vocab = vocab)\n",
    "\n",
    "    if save == True:\n",
    "        with open('dataloader.pkl', 'wb') as output:\n",
    "            pickle.dump(dataset, output, pickle.HIGHEST_PROTOCOL)\n",
    "     \n",
    "    total_batches = len(train_loader)\n",
    "    vocab_size = len(dataset.vocab)\n",
    "    \n",
    "    return train_loader, dataset, total_batches, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-cincinnati",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-porcelain",
   "metadata": {},
   "source": [
    "The idea is that: we want our model to mimic how we evaluate the relation, which is : first we remember the keyword and we see through the sentence to see if there is the pattern of our keyword in the sentence. \n",
    "\n",
    "The training steps:\n",
    "    1. We feed our keyword into the EncoderK to encode the keyword to a vector.\n",
    "    2. We concate the encoded keyword to every word of the sentence to make sure our model remember the keyword when reading the sentence (one can also use the Attention mechanism, but since our keyword are usually very short here, I don't think that is necessary)\n",
    "    3. We feed the concated sentence to the EncoderD to make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderK(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, p):\n",
    "        super(EncoderK, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, bidirectional=True,dropout = p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.embedding(x)\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        encodedK, (hidden, cell) = self.rnn(embedding)\n",
    "        #encoder_states:  (seq_len, N, 2* hidden_size)\n",
    "        #hidden:  (num_layers * 2, N, hidden_size)\n",
    "        #cell: (num_layers * 2, N, hidden_size)\n",
    "\n",
    "        return encodedK[-1,:,:], hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderD(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, p):\n",
    "        super(EncoderD, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size*3, hidden_size, num_layers, bidirectional=True,dropout = p)\n",
    "        self.ff = nn.Linear(hidden_size*2,2)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, encodedK,hidden, cell):\n",
    "        # x: (seq_length, N) where N is batch size\n",
    "        #encodedK:  (1, N, 2* hidden_size)\n",
    "        #hidden:  (num_layers * 2, N, hidden_size)\n",
    "        #cell: (num_layers * 2, N, hidden_size)\n",
    "        \n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # embedding shape: (seq_length, N, hidden_size)\n",
    "        encodedK = encodedK.repeat(embedding.shape[0],1,1)\n",
    "        embedding = torch.cat((embedding, encodedK),2)\n",
    "        \n",
    "\n",
    "        out, _ = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "        out = self.dropout(out[-1,:,:])\n",
    "        out = self.ff(out)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoders(new = False,model_num = 1):\n",
    "    \n",
    "    plot_losses = []\n",
    "    start_from = 0\n",
    "    \n",
    "    if new == True:\n",
    "        encoderK1 = EncoderK(vocab_size,hidden_size,num_layers,p).to(device)\n",
    "        encoderD1 = EncoderD(vocab_size,hidden_size,num_layers,p).to(device)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        with open('model'+str(model_num)+'.pkl', 'rb') as input:\n",
    "            encoderK1 = pickle.load(input)\n",
    "            encoderD1 = pickle.load(input)\n",
    "            plot_losses = pickle.load(input)\n",
    "            start_from = pickle.load(input)\n",
    "    return encoderK1, encoderD1, plot_losses, start_from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-soundtrack",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-photography",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(num_epochs = 1,plot_every = 500,print_every = 5000,save_every = 50000,start_from = 0):\n",
    "    \n",
    "    encoderD1.train()\n",
    "    encoderK1.train()\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        if num_epochs != 1:\n",
    "            print(\"epoch: \",ep+1)\n",
    "            print(\"=================================================================================================================\")\n",
    "        \n",
    "        global batch_count\n",
    "        batch_count = 0\n",
    "        plot_loss_total = 0\n",
    "        print_loss_total = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        for idx, (scores, keys, descs) in enumerate(train_loader):\n",
    "\n",
    "            encoderK_optimizer.zero_grad()\n",
    "            encoderD_optimizer.zero_grad()\n",
    "\n",
    "            keys = keys.to(device)\n",
    "            scores = scores.to(device)\n",
    "            descs = descs.to(device)\n",
    "\n",
    "\n",
    "            encodedK, hidden, cell = encoderK1(keys)\n",
    "\n",
    "\n",
    "            prediction = encoderD1(descs,encodedK,hidden,cell)\n",
    "\n",
    "            loss = criteria(prediction, scores)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            encoderK_optimizer.step()\n",
    "            encoderD_optimizer.step()\n",
    "\n",
    "\n",
    "            plot_loss_total += loss\n",
    "            print_loss_total += loss\n",
    "            batch_count+=1\n",
    "            \n",
    "            if batch_count % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "            if batch_count % print_every == 0:\n",
    "\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, (batch_count) / (total_batches)),batch_count+start_from, (batch_count+start_from) / (total_batches+start_from) * 100, print_loss_total/print_every))\n",
    "                print_loss_total = 0\n",
    "                \n",
    "            if batch_count % save_every == 0:\n",
    "                num = (batch_count//save_every)%2+1\n",
    "                with open('model'+str(num)+'.pkl', 'wb') as output:\n",
    "                    pickle.dump(encoderK1, output, pickle.HIGHEST_PROTOCOL)\n",
    "                    pickle.dump(encoderD1, output, pickle.HIGHEST_PROTOCOL)\n",
    "                    pickle.dump(plot_losses, output, pickle.HIGHEST_PROTOCOL)\n",
    "                    pickle.dump(batch_count+start_from, output, pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Saved model\"+str(num)+\".\\nBatch count: \",batch_count)\n",
    "                \n",
    "        with open('model'+str(num%2+1)+'.pkl', 'wb') as output:\n",
    "            pickle.dump(encoderK1, output, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(encoderD1, output, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(plot_losses, output, pickle.HIGHEST_PROTOCOL)\n",
    "            pickle.dump(0, output, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Saved model\"+str(num)+\". End of epoch \",ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-harvard",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader,encoderK1,encoderD1,threshold = 0.5,sample_size = 10000,print_index = False):\n",
    "    with torch.no_grad():\n",
    "        encoderK1.eval()\n",
    "        encoderD1.eval()\n",
    "        \n",
    "        right_num = 0\n",
    "        total_num = 0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(total = sample_size,position=0, leave=True)\n",
    "        \n",
    "        for idx, (scores, keys, descs) in enumerate(test_loader):\n",
    "            \n",
    "            \n",
    "            keys = keys.to(device)\n",
    "            scores = scores.to(device)\n",
    "            descs = descs.to(device)\n",
    "\n",
    "\n",
    "            encodedK, hidden, cell = encoderK1(keys)\n",
    "\n",
    "\n",
    "            prediction = encoderD1(descs,encodedK,hidden,cell)\n",
    "            \n",
    "            loss = criteria(prediction, scores)\n",
    "            \n",
    "            predic_prob = F.log_softmax(prediction)\n",
    "\n",
    "            if predic_prob[0][scores.item()].item() > math.log(threshold):\n",
    "                right_num += 1\n",
    "                total_num += 1\n",
    "                \n",
    "            else:\n",
    "                total_num += 1\n",
    "                if print_index == True:\n",
    "                    print(idx,predic_prob[0][scores.item()].item())\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pbar.update()\n",
    "            \n",
    "            if total_num == sample_size:\n",
    "                break\n",
    "            \n",
    "    encoderK1.train()\n",
    "    encoderD1.train()\n",
    "    \n",
    "    return total_loss/total_num, right_num, total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-copyright",
   "metadata": {},
   "source": [
    "# Start running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-growing",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "p = 0.2\n",
    "\n",
    "plot_every = 500 #unit: batchs\n",
    "print_every = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-teacher",
   "metadata": {},
   "source": [
    "### Create model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoderK1, encoderD1, plot_losses, start_from = get_encoders(new = True,model_num =1) # new = False for loading model using pickle\n",
    "\n",
    "encoderK_optimizer = optim.SGD(encoderK1.parameters(), lr=learning_rate,momentum=0.9)\n",
    "encoderD_optimizer = optim.SGD(encoderD1.parameters(), lr=learning_rate,momentum=0.9)\n",
    "criteria = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-prayer",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-course",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, dataset, total_batches, vocab_size = get_or_load_data(batch_size = batch_size,from_csv_file = False,save = False,start_from=start_from)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-postage",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-branch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(num_epochs = 1,plot_every = 5000,print_every = 50000,save_every = 500000,start_from = start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for saving model when training accidently ended\n",
    "\n",
    "num = 2\n",
    "\n",
    "with open('model'+str(num)+'.pkl', 'wb') as output:\n",
    "    pickle.dump(encoderK1, output, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(encoderD1, output, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(plot_losses, output, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(batch_count+start_from, output, pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved model\"+str(num)+\".\\nBatch count: \",batch_count+start_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-brief",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader, test_dataset, test_total_batches, _ = get_or_load_data(batch_size = 1,from_csv_file = True,save = False,csv_file = \"test.csv\",vocab = dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss, right_num, total_num = evaluate(test_loader,encoderK1,encoderD1,threshold = 0.5,sample_size = 5000,print_index=True)\n",
    "print(\"right: \",right_num,\"  out of: \",total_num,\"  Accuracy: \",right_num/total_num)\n",
    "print(\"avg loss: \",avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-picnic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
